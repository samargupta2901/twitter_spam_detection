{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83454090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error Tunnel connection\n",
      "[nltk_data]     failed: 407 Proxy Authentication Required>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,LancasterStemmer,WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8d298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv(\"spam.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9990616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.shape\n",
    "messages.drop(labels = ['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis = 1,inplace = True)\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda5a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to report confusion metrics\n",
    "\n",
    "def confusion_metrics (conf_matrix,y_test,y_pred):\n",
    "    from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "    print(\"Classification accuracy = \", accuracy_score(y_test,y_pred)*100)\n",
    "    print(\"Precision               = \", precision_score(y_test,y_pred)*100)\n",
    "    print(\"Recall                  = \", recall_score(y_test,y_pred)*100)\n",
    "    print(\"F1-score                = \", f1_score(y_test,y_pred)*100)\n",
    "    print(\"Neagtive recall         = \", (conf_matrix[0][0] / float(conf_matrix[0][0] + conf_matrix[0][1]))*100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37942a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to find the accuracy\n",
    "\n",
    "def accuracy_score (y_test,y_pred):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cef12e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "ss = SnowballStemmer(language='english')\n",
    "ls = LancasterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "corpus1 = []\n",
    "corpus2 = []\n",
    "corpus3 = []\n",
    "corpus4 = []\n",
    "\n",
    "for i in range(0,len(messages)):\n",
    "    data_clean = re.sub('[^a-zA-Z]', ' ', messages['v2'][i])\n",
    "    data_clean = data_clean.lower()\n",
    "    data_clean = data_clean.split()\n",
    "    \n",
    "    data_stem_ps = [ps.stem(word) for word in data_clean if not word in stopwords.words('english')]\n",
    "    data_stem_ss = [ss.stem(word) for word in data_clean if not word in stopwords.words('english')]\n",
    "    data_stem_ls = [ls.stem(word) for word in data_clean if not word in stopwords.words('english')]\n",
    "    \n",
    "    data_lem = [lem.lemmatize(word) for word in data_clean if not word in stopwords.words('english')]\n",
    "    data_lem = ' '.join(data_lem)\n",
    "    \n",
    "    data_lem_ps = [lem.lemmatize(word) for word in data_stem_ps if not word in stopwords.words('english')]\n",
    "    data_lem_ps = ' '.join(data_lem_ps)\n",
    "    data_lem_ss = [lem.lemmatize(word) for word in data_stem_ss if not word in stopwords.words('english')]\n",
    "    data_lem_ss = ' '.join(data_lem_ss)\n",
    "    \n",
    "    data_lem_ls = [lem.lemmatize(word) for word in data_stem_ls if not word in stopwords.words('english')]\n",
    "    data_lem_ls = ' '.join(data_lem_ls)\n",
    "    \n",
    "    corpus1.append(data_lem_ps)\n",
    "    corpus2.append(data_lem_ss)\n",
    "    corpus3.append(data_lem_ls)\n",
    "    corpus4.append(data_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d08b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the TF-IDF model\n",
    "def tfidf_model(corpus1,corpus2,corpus3,corpus4):\n",
    "    cv = TfidfVectorizer(max_features=5000)\n",
    "    X1 = cv.fit_transform(corpus1).toarray()\n",
    "    X2 = cv.fit_transform(corpus2).toarray()\n",
    "    X3 = cv.fit_transform(corpus3).toarray()\n",
    "    X4 = cv.fit_transform(corpus4).toarray()\n",
    "    Y = pd.get_dummies(messages['v1'])\n",
    "    Y=Y.iloc[:,1].values\n",
    "    return X1,X2,X3,X4,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "def bag_of_words_model(corpus1,corpus2,corpus3,corpus4):\n",
    "        cv = CountVectorizer(max_features=5000)\n",
    "        X1 = cv.fit_transform(corpus1).toarray()\n",
    "        X2 = cv.fit_transform(corpus2).toarray()\n",
    "        X3 = cv.fit_transform(corpus3).toarray()\n",
    "        X4 = cv.fit_transform(corpus4).toarray()\n",
    "        Y = pd.get_dummies(messages['v1'])\n",
    "        Y=Y.iloc[:,1].values\n",
    "        return X1,X2,X3,X4,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe11cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split for TF-IDF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1tf = tfidf_model(corpus1,corpus2,corpus3,corpus4)[0]\n",
    "X2tf = tfidf_model(corpus1,corpus2,corpus3,corpus4)[1]\n",
    "X3tf = tfidf_model(corpus1,corpus2,corpus3,corpus4)[2]\n",
    "X4tf = tfidf_model(corpus1,corpus2,corpus3,corpus4)[3]\n",
    "Ytf  = tfidf_model(corpus1,corpus2,corpus3,corpus4)[4]\n",
    "X1tf_train, X1tf_test, Y1tf_train, Y1tf_test = train_test_split(X1tf, Ytf, test_size = 0.30, random_state = 0)\n",
    "X2tf_train, X2tf_test, Y2tf_train, Y2tf_test = train_test_split(X2tf, Ytf, test_size = 0.30, random_state = 0)\n",
    "X3tf_train, X3tf_test, Y3tf_train, Y3tf_test = train_test_split(X3tf, Ytf, test_size = 0.30, random_state = 0)\n",
    "X4tf_train, X4tf_test, Y4tf_train, Y4tf_test = train_test_split(X4tf, Ytf, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split for Bag-of-words\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1bow = bag_of_words_model(corpus1,corpus2,corpus3,corpus4)[0]\n",
    "X2bow = bag_of_words_model(corpus1,corpus2,corpus3,corpus4)[1]\n",
    "X3bow = bag_of_words_model(corpus1,corpus2,corpus3,corpus4)[2]\n",
    "X4bow = bag_of_words_model(corpus1,corpus2,corpus3,corpus4)[3]\n",
    "Ybow  = bag_of_words_model(corpus1,corpus2,corpus3,corpus4)[4]\n",
    "X1bow_train, X1bow_test, Y1bow_train, Y1bow_test = train_test_split(X1bow, Ybow, test_size = 0.30, random_state = 0)\n",
    "X2bow_train, X2bow_test, Y2bow_train, Y2bow_test = train_test_split(X2bow, Ybow, test_size = 0.30, random_state = 0)\n",
    "X3bow_train, X3bow_test, Y3bow_train, Y3bow_test = train_test_split(X3bow, Ybow, test_size = 0.30, random_state = 0)\n",
    "X4bow_train, X4bow_test, Y4bow_train, Y4bow_test = train_test_split(X4bow, Ybow, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1tf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3feeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "def MNB_model(x_train, x_test, y_train, y_test):\n",
    "    MNB = MultinomialNB()\n",
    "    MNB.fit(x_train, y_train)\n",
    "    y_pred = MNB.predict(x_test)\n",
    "    score = accuracy_score(y_test,y_pred)\n",
    "    return score\n",
    "\n",
    "def MNB2_model(x_train, x_test, y_train, y_test):\n",
    "    MNB = MultinomialNB()\n",
    "    MNB.fit(x_train, y_train)\n",
    "    y_pred = MNB.predict(x_test)\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    return confusion_metrics(cm, y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ed1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes\n",
    "\n",
    "def BNB_model(x_train, x_test, y_train, y_test):\n",
    "    BNB = BernoulliNB()\n",
    "    BNB.fit(x_train, y_train)\n",
    "    y_pred = BNB.predict(x_test)\n",
    "    score = accuracy_score(y_test,y_pred)\n",
    "    return score\n",
    "\n",
    "def BNB2_model(x_train, x_test, y_train, y_test):\n",
    "    BNB = BernoulliNB()\n",
    "    BNB.fit(x_train, y_train)\n",
    "    y_pred = BNB.predict(x_test)\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    return confusion_metrics(cm, y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe19e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "def SVM_model(x_train, x_test, y_train, y_test):\n",
    "    SVM = LinearSVC()\n",
    "    SVM.fit(x_train, y_train)\n",
    "    y_pred = SVM.predict(x_test)\n",
    "    score = accuracy_score(y_test,y_pred)\n",
    "    return score\n",
    "\n",
    "def SVM2_model(x_train, x_test, y_train, y_test):\n",
    "    SVM = LinearSVC()\n",
    "    SVM.fit(x_train, y_train)\n",
    "    y_pred = SVM.predict(x_test)\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    return confusion_metrics(cm, y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46fb456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree Classifier\n",
    "\n",
    "def DT_model(x_train, x_test, y_train, y_test):\n",
    "    DT = DecisionTreeClassifier()\n",
    "    DT.fit(x_train, y_train)\n",
    "    y_pred = DT.predict(x_test)\n",
    "    score = accuracy_score(y_test,y_pred)\n",
    "    return score\n",
    "\n",
    "def DT2_model(x_train, x_test, y_train, y_test):\n",
    "    DT = DecisionTreeClassifier()\n",
    "    DT.fit(x_train, y_train)\n",
    "    y_pred = DT.predict(x_test)\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    return confusion_metrics(cm, y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046daab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest classifier\n",
    "\n",
    "def RF_model(x_train, x_test, y_train, y_test):\n",
    "    RF = RandomForestClassifier()\n",
    "    RF.fit(x_train, y_train)\n",
    "    y_pred = RF.predict(x_test)\n",
    "    score = accuracy_score(y_test,y_pred)\n",
    "    return score\n",
    "\n",
    "def RF2_model(x_train, x_test, y_train, y_test):\n",
    "    RF = RandomForestClassifier()\n",
    "    RF.fit(x_train, y_train)\n",
    "    y_pred = RF.predict(x_test)\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    return confusion_metrics(cm, y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "def LR_model(x_train, x_test, y_train, y_test):\n",
    "    LR = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "    LR.fit(x_train, y_train)\n",
    "    y_pred = LR.predict(x_test)\n",
    "    score = accuracy_score(y_test,y_pred)\n",
    "    return score\n",
    "\n",
    "def LR2_model(x_train, x_test, y_train, y_test):\n",
    "    LR = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "    LR.fit(x_train, y_train)\n",
    "    y_pred = LR.predict(x_test)\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    return confusion_metrics(cm, y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Measures after applying TF-IDF vectorizer\n",
    "\n",
    "print('Performance Measures after applying TF-IDF vectorizer')\n",
    "\n",
    "print('\\nMultinomial Naive Bayes :')\n",
    "MNB2_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)\n",
    "\n",
    "print('\\nBernoulli Naive Bayes :')\n",
    "BNB2_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)\n",
    "\n",
    "print('\\nSupport Vector Machine :')\n",
    "SVM2_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)\n",
    "\n",
    "print('\\nDecision Tree Classifier :')\n",
    "DT2_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)\n",
    "\n",
    "print('\\nRandom Forest Classifier :')\n",
    "RF2_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)\n",
    "\n",
    "print('\\nLogistic Regression :')\n",
    "LR2_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fbe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Measures after applying Bag of Words vectorizer\n",
    "\n",
    "print('Performance Measures after applying Bag of Words vectorizer','\\n')\n",
    "\n",
    "print('\\nMultinomial Naive Bayes :')\n",
    "MNB2_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)\n",
    "\n",
    "print('\\nBernoulli Naive Bayes :')\n",
    "BNB2_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)\n",
    "\n",
    "print('\\nSupport Vector Machine :')\n",
    "SVM2_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)\n",
    "\n",
    "print('\\nDecision Tree Classifier :')\n",
    "DT2_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)\n",
    "\n",
    "print('\\nRandom Forest Classifier :')\n",
    "RF2_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)\n",
    "\n",
    "print('\\nLogistic Regression :')\n",
    "LR2_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Measures for different stemmers and lemmatizer using BoW model\n",
    "\n",
    "print('Accuracy Measures for different stemmers and lemmatizer using BoW model','\\n')\n",
    "\n",
    "print('Porter Stemmer\\n')\n",
    "\n",
    "print('Multinomial Naive Bayes = ', MNB_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)*100)\n",
    "print('Bernoulli Naive Bayes   = ', BNB_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)*100)\n",
    "print('Support Vector Machine  = ', SVM_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)*100)\n",
    "print('Decision Tree           = ', DT_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)*100)\n",
    "print('Random Forest           = ', RF_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)*100)\n",
    "print('Logistic Regression     = ', LR_model(X1bow_train, X1bow_test, Y1bow_train, Y1bow_test)*100)\n",
    "\n",
    "\n",
    "print('\\n\\nSnowball Stemmer\\n')\n",
    "\n",
    "print('Multinomial Naive Bayes = ', MNB_model(X2bow_train, X2bow_test, Y2bow_train, Y2bow_test)*100)\n",
    "print('Bernoulli Naive Bayes   = ', BNB_model(X2bow_train, X2bow_test, Y2bow_train, Y2bow_test)*100)\n",
    "print('Support Vector Machine  = ', SVM_model(X2bow_train, X2bow_test, Y2bow_train, Y2bow_test)*100)\n",
    "print('Decision Tree           = ', DT_model(X2bow_train, X2bow_test, Y2bow_train, Y2bow_test)*100)\n",
    "print('Random Forest           = ', RF_model(X2bow_train, X2bow_test, Y2bow_train, Y2bow_test)*100)\n",
    "print('Logistic Regression     = ', LR_model(X2bow_train, X2bow_test, Y2bow_train, Y2bow_test)*100)\n",
    "\n",
    "\n",
    "print('\\n\\nLancaster Stemmer\\n')\n",
    "\n",
    "print('Multinomial Naive Bayes = ', MNB_model(X3bow_train, X3bow_test, Y3bow_train, Y3bow_test)*100)\n",
    "print('Bernoulli Naive Bayes   = ', BNB_model(X3bow_train, X3bow_test, Y3bow_train, Y3bow_test)*100)\n",
    "print('Support Vector Machine  = ', SVM_model(X3bow_train, X3bow_test, Y3bow_train, Y3bow_test)*100)\n",
    "print('Decision Tree           = ', DT_model(X3bow_train, X3bow_test, Y3bow_train, Y3bow_test)*100)\n",
    "print('Random Forest           = ', RF_model(X3bow_train, X3bow_test, Y3bow_train, Y3bow_test)*100)\n",
    "print('Logistic Regression     = ', LR_model(X3bow_train, X3bow_test, Y3bow_train, Y3bow_test)*100)\n",
    "\n",
    "\n",
    "print('\\n\\nLemmatizer\\n')\n",
    "\n",
    "print('Multinomial Naive Bayes = ', MNB_model(X4bow_train, X4bow_test, Y4bow_train, Y4bow_test)*100)\n",
    "print('Bernoulli Naive Bayes   = ', BNB_model(X4bow_train, X4bow_test, Y4bow_train, Y4bow_test)*100)\n",
    "print('Support Vector Machine  = ', SVM_model(X4bow_train, X4bow_test, Y4bow_train, Y4bow_test)*100)\n",
    "print('Decision Tree           = ', DT_model(X4bow_train, X4bow_test, Y4bow_train, Y4bow_test)*100)\n",
    "print('Random Forest           = ', RF_model(X4bow_train, X4bow_test, Y4bow_train, Y4bow_test)*100)\n",
    "print('Logistic Regression     = ', LR_model(X4bow_train, X4bow_test, Y4bow_train, Y4bow_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Measures for different stemmers and lemmatizer using TF-IDF model\n",
    "\n",
    "print('Accuracy Measures for different stemmers and lemmatizer using TF-IDF model','\\n')\n",
    "\n",
    "print('Porter Stemmer\\n')\n",
    "\n",
    "print('Multinomial Naive Bayes = ', MNB_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)*100)\n",
    "print('Bernoulli Naive Bayes   = ', BNB_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)*100)\n",
    "print('Support Vector Machine  = ', SVM_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)*100)\n",
    "print('Decision Tree           = ', DT_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)*100)\n",
    "print('Random Forest           = ', RF_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)*100)\n",
    "print('Logistic Regression     = ', LR_model(X1tf_train, X1tf_test, Y1tf_train, Y1tf_test)*100)\n",
    "\n",
    "\n",
    "print('\\n\\nSnowball Stemmer\\n')\n",
    "\n",
    "print('Multinomial Naive Bayes = ', MNB_model(X2tf_train, X2tf_test, Y2tf_train, Y2tf_test)*100)\n",
    "print('Bernoulli Naive Bayes   = ', BNB_model(X2tf_train, X2tf_test, Y2tf_train, Y2tf_test)*100)\n",
    "print('Support Vector Machine  = ', SVM_model(X2tf_train, X2tf_test, Y2tf_train, Y2tf_test)*100)\n",
    "print('Decision Tree           = ', DT_model(X2tf_train, X2tf_test, Y2tf_train, Y2tf_test)*100)\n",
    "print('Random Forest           = ', RF_model(X2tf_train, X2tf_test, Y2tf_train, Y2tf_test)*100)\n",
    "print('Logistic Regression     = ', LR_model(X2tf_train, X2tf_test, Y2tf_train, Y2tf_test)*100)\n",
    "\n",
    "\n",
    "print('\\n\\nLancaster Stemmer\\n')\n",
    "\n",
    "print('Multinomial Naive Bayes = ', MNB_model(X3tf_train, X3tf_test, Y3tf_train, Y3tf_test)*100)\n",
    "print('Bernoulli Naive Bayes   = ', BNB_model(X3tf_train, X3tf_test, Y3tf_train, Y3tf_test)*100)\n",
    "print('Support Vector Machine  = ', SVM_model(X3tf_train, X3tf_test, Y3tf_train, Y3tf_test)*100)\n",
    "print('Decision Tree           = ', DT_model(X3tf_train, X3tf_test, Y3tf_train, Y3tf_test)*100)\n",
    "print('Random Forest           = ', RF_model(X3tf_train, X3tf_test, Y3tf_train, Y3tf_test)*100)\n",
    "print('Logistic Regression     = ', LR_model(X3tf_train, X3tf_test, Y3tf_train, Y3tf_test)*100)\n",
    "\n",
    "\n",
    "print('\\n\\nLemmatizer\\n')\n",
    "\n",
    "print('Multinomial Naive Bayes = ', MNB_model(X4tf_train, X4tf_test, Y4tf_train, Y4tf_test)*100)\n",
    "print('Bernoulli Naive Bayes   = ', BNB_model(X4tf_train, X4tf_test, Y4tf_train, Y4tf_test)*100)\n",
    "print('Support Vector Machine  = ', SVM_model(X4tf_train, X4tf_test, Y4tf_train, Y4tf_test)*100)\n",
    "print('Decision Tree           = ', DT_model(X4tf_train, X4tf_test, Y4tf_train, Y4tf_test)*100)\n",
    "print('Random Forest           = ', RF_model(X4tf_train, X4tf_test, Y4tf_train, Y4tf_test)*100)\n",
    "print('Logistic Regression     = ', LR_model(X4tf_train, X4tf_test, Y4tf_train, Y4tf_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5486130",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "BNB = BernoulliNB()\n",
    "SVM = LinearSVC()\n",
    "DT = DecisionTreeClassifier()\n",
    "RFC = RandomForestClassifier()\n",
    "LR = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "classifiers = [MNB,BNB,SVM,DT,RFC, LR]\n",
    "Classifiers = ['Multinomial Na√Øve Bayes','Bernoulli Naive Bayes','Support Vector Machine','Decision tree classifier','Random Forest classifier','Logistic Regression']\n",
    "\n",
    "predictions_stem_ps_tf = []\n",
    "predictions_stem_ss_tf = []\n",
    "predictions_stem_ls_tf = []\n",
    "predictions_lem_tf = []\n",
    "\n",
    "predictions_stem_ps_bow = []\n",
    "predictions_stem_ss_bow = []\n",
    "predictions_stem_ls_bow = []\n",
    "predictions_lem_bow = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    \n",
    "    spamming_detection_model_stem_ps_tf = classifier.fit(X1tf_train, Y1tf_train)\n",
    "    spamming_detection_model_stem_ss_tf = classifier.fit(X2tf_train, Y2tf_train)\n",
    "    spamming_detection_model_stem_ls_tf = classifier.fit(X3tf_train, Y3tf_train)\n",
    "    spamming_detection_model_lem_tf = classifier.fit(X4tf_train, Y4tf_train)\n",
    "    \n",
    "    spamming_detection_model_stem_ps_bow = classifier.fit(X1bow_train, Y1bow_train)\n",
    "    spamming_detection_model_stem_ss_bow = classifier.fit(X2bow_train, Y2bow_train)\n",
    "    spamming_detection_model_stem_ls_bow = classifier.fit(X3bow_train, Y3bow_train)\n",
    "    spamming_detection_model_lem_bow = classifier.fit(X4bow_train, Y4bow_train)\n",
    "    \n",
    "    Y1_pred_tf=spamming_detection_model_stem_ps_tf.predict(X1tf_test)\n",
    "    Y2_pred_tf=spamming_detection_model_stem_ss_tf.predict(X2tf_test)\n",
    "    Y3_pred_tf=spamming_detection_model_stem_ls_tf.predict(X3tf_test)\n",
    "    Y4_pred_tf=spamming_detection_model_lem_tf.predict(X4tf_test)\n",
    "    \n",
    "    Y1_pred_bow=spamming_detection_model_stem_ps_bow.predict(X1bow_test)\n",
    "    Y2_pred_bow=spamming_detection_model_stem_ss_bow.predict(X2bow_test)\n",
    "    Y3_pred_bow=spamming_detection_model_stem_ls_bow.predict(X3bow_test)\n",
    "    Y4_pred_bow=spamming_detection_model_lem_bow.predict(X4bow_test)\n",
    "    \n",
    "    predictions_stem_ps_tf.append((Y1_pred_tf,Classifiers[classifiers.index(classifier)]))\n",
    "    predictions_stem_ss_tf.append((Y2_pred_tf,Classifiers[classifiers.index(classifier)]))\n",
    "    predictions_stem_ls_tf.append((Y3_pred_tf,Classifiers[classifiers.index(classifier)]))\n",
    "    predictions_lem_tf.append((Y4_pred_tf,Classifiers[classifiers.index(classifier)]))\n",
    "    \n",
    "    predictions_stem_ps_bow.append((Y1_pred_bow,Classifiers[classifiers.index(classifier)]))\n",
    "    predictions_stem_ss_bow.append((Y2_pred_bow,Classifiers[classifiers.index(classifier)]))\n",
    "    predictions_stem_ls_bow.append((Y3_pred_bow,Classifiers[classifiers.index(classifier)]))\n",
    "    predictions_lem_bow.append((Y4_pred_bow,Classifiers[classifiers.index(classifier)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy Score for TF-IDF','\\n\\n')\n",
    "\n",
    "for i in range(len(predictions_stem_ps_tf)):\n",
    "    print('\\n',predictions_stem_ps_tf[i][1],' - Porter','\\n')\n",
    "    accuracy_score(Y1tf_test,predictions_stem_ps_tf[i][0])\n",
    "    \n",
    "for i in range(len(predictions_stem_ss_tf)):\n",
    "    print('\\n',predictions_stem_ss_tf[i][1],' - Snowball','\\n')\n",
    "    accuracy_score(Y2tf_test,predictions_stem_ss_tf[i][0])\n",
    "    \n",
    "for i in range(len(predictions_stem_ls_tf)):\n",
    "    print('\\n',predictions_stem_ls_tf[i][1],' - Lancaster','\\n')\n",
    "    accuracy_score(Y3tf_test,predictions_stem_ls_tf[i][0])\n",
    "    \n",
    "for i in range(len(predictions_lem_tf)):\n",
    "    print('\\n',predictions_lem_tf[i][1],' - Lemmatizer','\\n')\n",
    "    accuracy_score(Y4tf_test,predictions_lem_tf[i][0])\n",
    "    \n",
    "print('\\n','Accuracy Score for Bag of words','\\n')\n",
    "\n",
    "for i in range(len(predictions_stem_ps_bow)):\n",
    "    print('\\n',predictions_stem_ps_bow[i][1],' - Porter','\\n')\n",
    "    accuracy_score(Y1bow_test,predictions_stem_ps_bow[i][0])\n",
    "    \n",
    "for i in range(len(predictions_stem_ss_bow)):\n",
    "    print('\\n',predictions_stem_ss_bow[i][1],' - Snowball','\\n')\n",
    "    accuracy_score(Y2bow_test,predictions_stem_ss_bow[i][0])\n",
    "    \n",
    "for i in range(len(predictions_stem_ls_bow)):\n",
    "    print('\\n',predictions_stem_ls_bow[i][1],' - Lancaster','\\n')\n",
    "    accuracy_score(Y3bow_test,predictions_stem_ls_bow[i][0])\n",
    "    \n",
    "for i in range(len(predictions_lem_bow)):\n",
    "    print('\\n',predictions_lem_bow[i][1],' - Lemmatizer','\\n')\n",
    "    accuracy_score(Y4bow_test,predictions_lem_bow[i][0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions_stem_ps)):\n",
    "    print('\\n',predictions_stem_ps[i][1],' - Porter','\\n')\n",
    "    confusion_m=confusion_matrix(predictions_stem_ps[i][0],Y1_test)\n",
    "    print('Confusion matrix', '\\n',confusion_m,'\\n')\n",
    "    accuracy_score(Y1_test,predictions_stem_ps[i][0])\n",
    "    \n",
    "for i in range(len(predictions_stem_ss)):\n",
    "    print('\\n',predictions_stem_ss[i][1],' - Snowball','\\n')\n",
    "    confusion_m=confusion_matrix(predictions_stem_ss[i][0],Y2_test)\n",
    "    print('Confusion matrix', '\\n',confusion_m,'\\n')\n",
    "    accuracy_score(Y2_test,predictions_stem_ss[i][0])\n",
    "    \n",
    "for i in range(len(predictions_stem_ls)):\n",
    "    print('\\n',predictions_stem_ls[i][1],' - Lancaster','\\n')\n",
    "    confusion_m=confusion_matrix(predictions_stem_ls[i][0],Y3_test)\n",
    "    print('Confusion matrix', '\\n',confusion_m,'\\n')\n",
    "    accuracy_score(Y3_test,predictions_stem_ls[i][0])\n",
    "    \n",
    "for i in range(len(predictions_lem)):\n",
    "    print('\\n',predictions_lem[i][1],' - Lemmatizer','\\n')\n",
    "    confusion_m=confusion_matrix(predictions_lem[i][0],Y4_test)\n",
    "    print('Confusion matrix', '\\n',confusion_m,'\\n')\n",
    "    accuracy_score(Y4_test,predictions_lem[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30872314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
